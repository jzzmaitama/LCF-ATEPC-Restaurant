{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db05a290a2fb23cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:24:49.969886Z",
     "start_time": "2024-07-14T12:24:49.908856Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Annotated ABSA with Emotions Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13756aa288d4e200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:24:50.024263Z",
     "start_time": "2024-07-14T12:24:49.973881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4832 entries, 0 to 4831\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Review Sentence        4832 non-null   object \n",
      " 1   Aspect term            4832 non-null   object \n",
      " 2   polarity               4832 non-null   object \n",
      " 3   from                   4832 non-null   int64  \n",
      " 4   to                     4832 non-null   int64  \n",
      " 5   Anger                  4830 non-null   float64\n",
      " 6   Disgust                4831 non-null   float64\n",
      " 7   Fear                   4826 non-null   float64\n",
      " 8   Joy                    4815 non-null   float64\n",
      " 9   Sadness                4830 non-null   float64\n",
      " 10  Surprise               4823 non-null   float64\n",
      " 11  Emotion Class          4832 non-null   object \n",
      " 12  emotion context words  4671 non-null   object \n",
      "dtypes: float64(6), int64(2), object(5)\n",
      "memory usage: 490.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25bcee935323a96f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T11:09:28.666096Z",
     "start_time": "2024-07-08T11:09:28.654883Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_emotions(emotion):\n",
    "    if emotion in ['Anger', 'Disgust', 'Fear', 'Sadness']:\n",
    "        return 'Anger'\n",
    "    elif emotion == 'Joy':\n",
    "        return 'Joy'\n",
    "    else:  # 'Surprise'\n",
    "        return 'Surprise'\n",
    "\n",
    "# Apply the function to the 'Emotion Class' column\n",
    "data['Emotion Class'] = data['Emotion Class'].apply(merge_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63797c5984a2cf85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:24:50.057668Z",
     "start_time": "2024-07-14T12:24:50.030254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Emotion Class\nJoy         3132\nAnger        961\nDisgust      364\nSurprise     199\nFear         118\nSadness       58\nName: count, dtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Emotion Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d7f9804d201a21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:25:05.356328Z",
     "start_time": "2024-07-14T12:25:05.342364Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop the Polarity  row with the value \"conflict\"\n",
    "data = data[data['polarity'] != 'conflict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450741f6e99d731d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:25:11.693546Z",
     "start_time": "2024-07-14T12:25:11.671531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "polarity\npositive    2891\nnegative    1003\nneutral      833\nName: count, dtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:25:17.084925Z",
     "start_time": "2024-07-14T12:25:17.075475Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_all_aspects(data):\n",
    "    aspect_dict = {}\n",
    "    for _, row in data.iterrows():\n",
    "        sentence = row['Review Sentence']\n",
    "        aspect = row['Aspect term']\n",
    "        if sentence not in aspect_dict:\n",
    "            aspect_dict[sentence] = []\n",
    "        aspect_dict[sentence].append(aspect)\n",
    "    return aspect_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69858c8f2c8e300e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T11:10:10.743822Z",
     "start_time": "2024-07-08T11:10:10.732776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "list(string.digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb3aa4597bf8327e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:25:22.542605Z",
     "start_time": "2024-07-14T12:25:22.430281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                        Review Sentence           Aspect term  \\\n0                  But the staff was so horrible to us.                 staff   \n1     To be completely fair, the only redeeming fact...                  food   \n2     The food is uniformly exceptional, with a very...                  food   \n3     The food is uniformly exceptional, with a very...               kitchen   \n4     The food is uniformly exceptional, with a very...                  menu   \n...                                                 ...                   ...   \n4827  Each table has a pot of boiling water sunken i...  pot of boiling water   \n4828  Each table has a pot of boiling water sunken i...                 meats   \n4829  Each table has a pot of boiling water sunken i...            vegetables   \n4830  Each table has a pot of boiling water sunken i...                  rice   \n4831  Each table has a pot of boiling water sunken i...         glass noodles   \n\n      polarity  from   to  Anger  Disgust  Fear  Joy  Sadness  Surprise  \\\n0     negative     8   13    4.0      1.0   1.0  0.0      3.0       1.0   \n1     positive    57   61    0.0      0.0   0.0  2.0      0.0       0.0   \n2     positive     4    8    0.0      0.0   0.0  4.0      0.0       0.0   \n3     positive    55   62    0.0      0.0   0.0  4.0      0.0       0.0   \n4      neutral   141  145    0.0      0.0   0.0  4.0      0.0       0.0   \n...        ...   ...  ...    ...      ...   ...  ...      ...       ...   \n4827   neutral    17   37    2.0      2.0   0.0  1.0      0.0       0.0   \n4828   neutral    99  104    0.0      0.0   0.0  3.0      0.0       1.0   \n4829   neutral   114  124    0.0      0.0   0.0  2.0      0.0       1.0   \n4830   neutral   130  134    0.0      0.0   0.0  2.0      0.0       1.0   \n4831   neutral   139  152    0.0      0.0   0.0  2.0      0.0       1.0   \n\n     Emotion Class                              emotion context words  \n0            Anger                                           horrible  \n1              Joy                            which was above average  \n2              Joy        uniformly exceptional, very capable proudly  \n3              Joy          uniformly exceptional very capableproudly  \n4              Joy           uniformly exceptionalvery capableproudly  \n...            ...                                                ...  \n4827         Anger  sunken into its surface(1)you get platters of ...  \n4828           Joy  sunken into its surface(1)you get platters of ...  \n4829           Joy  sunken into its surface(1)you get platters of ...  \n4830           Joy  sunken into its surface(1)you get platters of ...  \n4831           Joy  sunken into its surface(1)you get platters of ...  \n\n[4727 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Review Sentence</th>\n      <th>Aspect term</th>\n      <th>polarity</th>\n      <th>from</th>\n      <th>to</th>\n      <th>Anger</th>\n      <th>Disgust</th>\n      <th>Fear</th>\n      <th>Joy</th>\n      <th>Sadness</th>\n      <th>Surprise</th>\n      <th>Emotion Class</th>\n      <th>emotion context words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>But the staff was so horrible to us.</td>\n      <td>staff</td>\n      <td>negative</td>\n      <td>8</td>\n      <td>13</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>Anger</td>\n      <td>horrible</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>To be completely fair, the only redeeming fact...</td>\n      <td>food</td>\n      <td>positive</td>\n      <td>57</td>\n      <td>61</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Joy</td>\n      <td>which was above average</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The food is uniformly exceptional, with a very...</td>\n      <td>food</td>\n      <td>positive</td>\n      <td>4</td>\n      <td>8</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Joy</td>\n      <td>uniformly exceptional, very capable proudly</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The food is uniformly exceptional, with a very...</td>\n      <td>kitchen</td>\n      <td>positive</td>\n      <td>55</td>\n      <td>62</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Joy</td>\n      <td>uniformly exceptional very capableproudly</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The food is uniformly exceptional, with a very...</td>\n      <td>menu</td>\n      <td>neutral</td>\n      <td>141</td>\n      <td>145</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Joy</td>\n      <td>uniformly exceptionalvery capableproudly</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4827</th>\n      <td>Each table has a pot of boiling water sunken i...</td>\n      <td>pot of boiling water</td>\n      <td>neutral</td>\n      <td>17</td>\n      <td>37</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Anger</td>\n      <td>sunken into its surface(1)you get platters of ...</td>\n    </tr>\n    <tr>\n      <th>4828</th>\n      <td>Each table has a pot of boiling water sunken i...</td>\n      <td>meats</td>\n      <td>neutral</td>\n      <td>99</td>\n      <td>104</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Joy</td>\n      <td>sunken into its surface(1)you get platters of ...</td>\n    </tr>\n    <tr>\n      <th>4829</th>\n      <td>Each table has a pot of boiling water sunken i...</td>\n      <td>vegetables</td>\n      <td>neutral</td>\n      <td>114</td>\n      <td>124</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Joy</td>\n      <td>sunken into its surface(1)you get platters of ...</td>\n    </tr>\n    <tr>\n      <th>4830</th>\n      <td>Each table has a pot of boiling water sunken i...</td>\n      <td>rice</td>\n      <td>neutral</td>\n      <td>130</td>\n      <td>134</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Joy</td>\n      <td>sunken into its surface(1)you get platters of ...</td>\n    </tr>\n    <tr>\n      <th>4831</th>\n      <td>Each table has a pot of boiling water sunken i...</td>\n      <td>glass noodles</td>\n      <td>neutral</td>\n      <td>139</td>\n      <td>152</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Joy</td>\n      <td>sunken into its surface(1)you get platters of ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>4727 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f2b84e08ce50be7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T16:55:02.759890Z",
     "start_time": "2024-06-30T16:54:54.083568Z"
    }
   },
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# \n",
    "# # Split the data into training and testing sets\n",
    "# train_df, test_df = train_test_split(data, test_size=0.20, random_state=42,shuffle=True)\n",
    "# \n",
    "# # Define the target variable for the training data\n",
    "# y_train = train_df['Emotion Class']\n",
    "# \n",
    "# # Instantiate the RandomOverSampler\n",
    "# ros = RandomOverSampler()\n",
    "# \n",
    "# # Resample the training dataset\n",
    "# X_resampled, y_resampled = ros.fit_resample(train_df, y_train)\n",
    "# \n",
    "# # Now, 'X_resampled' is your DataFrame with balanced 'Emotion Class' for the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdfbe328dc692745",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:25:44.329449Z",
     "start_time": "2024-07-14T12:25:37.016930Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "excluded_words = set(stopwords.words('english') + list(string.punctuation) + list(string.digits))\n",
    "\n",
    "\n",
    "# new implementation\n",
    "def nltk_format_data(row, all_aspects):\n",
    "    # Replace NaN values with empty strings\n",
    "    row = row.fillna('')\n",
    "    \n",
    "    # Remove numbers\n",
    "    sentence = re.sub(r'\\(\\d+\\)', '', row['Review Sentence'])\n",
    "\n",
    "    # Tokenize the sentence while correctly handling punctuation\n",
    "    tokens = word_tokenize(sentence)\n",
    "    current_aspect = row['Aspect term']\n",
    "    polarity_code = {'positive': 2, 'neutral': 0, 'negative': 1}[row['polarity']]\n",
    "\n",
    "    # Map emotion classes to numerical codes\n",
    "    emotion_code = {'Anger': 0, 'Disgust': 1, 'Fear': 2, 'Joy': 3, 'Sadness': 4, 'Surprise': 5}\n",
    "    # emotion_code = {'Anger': 0, 'Joy': 1, 'Surprise': 2}\n",
    "    \n",
    "    # Get all aspects for the current sentence\n",
    "    all_aspects_in_sentence = all_aspects[row['Review Sentence']]\n",
    "\n",
    "    # Normalize the tokens to ensure consistent matching with aspect terms\n",
    "    normalized_tokens = [token.rstrip('.,?!:;') for token in tokens]\n",
    "\n",
    "    # Get the emotion words for the current sentence\n",
    "    emotion_words = word_tokenize(row['emotion context words'])  \n",
    "    emotion_words = [word for word in emotion_words if word not in excluded_words]\n",
    "\n",
    "    formatted_sentence = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        normalized_token = normalized_tokens[i]\n",
    "        matched_aspect = None\n",
    "\n",
    "        # Check if the token is part of any aspect term\n",
    "        for aspect in all_aspects_in_sentence:\n",
    "            aspect_tokens = aspect.split()\n",
    "            aspect_length = len(aspect_tokens)\n",
    "            if normalized_tokens[i:i+aspect_length] == aspect_tokens:\n",
    "                matched_aspect = aspect\n",
    "                break\n",
    "\n",
    "        if matched_aspect:\n",
    "            aspect_tokens = matched_aspect.split()\n",
    "            for j, aspect_token in enumerate(aspect_tokens):\n",
    "                if j == 0:\n",
    "                    formatted_sentence.append(f\"{aspect_token} B-ASP {polarity_code if matched_aspect == current_aspect else '-1'} {emotion_code[row['Emotion Class']] if matched_aspect == current_aspect else '-1'}\")\n",
    "                else:\n",
    "                    formatted_sentence.append(f\"{aspect_token} I-ASP {polarity_code if matched_aspect == current_aspect else '-1'} {emotion_code[row['Emotion Class']] if matched_aspect == current_aspect else '-1'}\")\n",
    "            i += len(aspect_tokens)  # Skip the aspect tokens\n",
    "        else:\n",
    "            # Check if the token is part of the emotion words\n",
    "            if normalized_token in emotion_words:\n",
    "                formatted_sentence.append(f\"{tokens[i]} O -1 {emotion_code[row['Emotion Class']]}\")\n",
    "            else:\n",
    "                formatted_sentence.append(f\"{tokens[i]} O -1 -1\")\n",
    "            i += 1\n",
    "\n",
    "    return '\\n'.join(formatted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4323e3aa9a5ca5ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:25:58.249894Z",
     "start_time": "2024-07-14T12:25:48.644887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       But O -1 -1\\nthe O -1 -1\\nstaff B-ASP 1 0\\nwas...\n",
      "1       To O -1 -1\\nbe O -1 -1\\ncompletely O -1 -1\\nfa...\n",
      "2       The O -1 -1\\nfood B-ASP 2 3\\nis O -1 -1\\nunifo...\n",
      "3       The O -1 -1\\nfood B-ASP -1 -1\\nis O -1 -1\\nuni...\n",
      "4       The O -1 -1\\nfood B-ASP -1 -1\\nis O -1 -1\\nuni...\n",
      "                              ...                        \n",
      "4827    Each O -1 -1\\ntable B-ASP -1 -1\\nhas O -1 -1\\n...\n",
      "4828    Each O -1 -1\\ntable B-ASP -1 -1\\nhas O -1 -1\\n...\n",
      "4829    Each O -1 -1\\ntable B-ASP -1 -1\\nhas O -1 -1\\n...\n",
      "4830    Each O -1 -1\\ntable B-ASP -1 -1\\nhas O -1 -1\\n...\n",
      "4831    Each O -1 -1\\ntable B-ASP -1 -1\\nhas O -1 -1\\n...\n",
      "Length: 4727, dtype: object\n",
      "To O -1 -1\n",
      "be O -1 -1\n",
      "completely O -1 -1\n",
      "fair O -1 -1\n",
      ", O -1 -1\n",
      "the O -1 -1\n",
      "only O -1 -1\n",
      "redeeming O -1 -1\n",
      "factor O -1 -1\n",
      "was O -1 -1\n",
      "the O -1 -1\n",
      "food B-ASP 2 3\n",
      ", O -1 -1\n",
      "which O -1 -1\n",
      "was O -1 -1\n",
      "above O -1 -1\n",
      "average O -1 3\n",
      ", O -1 -1\n",
      "but O -1 -1\n",
      "could O -1 -1\n",
      "n't O -1 -1\n",
      "make O -1 -1\n",
      "up O -1 -1\n",
      "for O -1 -1\n",
      "all O -1 -1\n",
      "the O -1 -1\n",
      "other O -1 -1\n",
      "deficiencies O -1 -1\n",
      "of O -1 -1\n",
      "Teodora O -1 -1\n",
      ". O -1 -1\n"
     ]
    }
   ],
   "source": [
    "# Get all aspects for each sentence\n",
    "all_aspects = get_all_aspects(data)\n",
    "\n",
    "# Apply NLTK formatting to the DataFrame and display the results\n",
    "example_data_nltk_formatted = data.apply(nltk_format_data, axis=1, all_aspects=all_aspects)\n",
    "print(example_data_nltk_formatted)  # Displaying formatted data for the first entry\n",
    "print(example_data_nltk_formatted.values[1])  # Displaying formatted data for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ada652ad15304bf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T16:57:02.896911Z",
     "start_time": "2024-06-30T16:57:02.859265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "7302"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a08512e6eef041",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:26:18.841989Z",
     "start_time": "2024-07-14T12:26:18.825749Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(example_data_nltk_formatted, test_size=0.30, random_state=42, shuffle=True)\n",
    "\n",
    "# # Calculate the number of rows for training and testing\n",
    "# train_size = int(0.80 * len(example_data_nltk_formatted))\n",
    "# test_size = len(example_data_nltk_formatted) - train_size\n",
    "# \n",
    "# #Split the data into training and testing sets\n",
    "# train_df = example_data_nltk_formatted.iloc[:train_size]\n",
    "# test_df = example_data_nltk_formatted.iloc[train_size:]\n",
    "\n",
    "\n",
    "# # Add a new line at the end of each sentence\n",
    "# train_df = train_df.apply(lambda x: x + '\\n' if isinstance(x, str) else x)\n",
    "# test_df = test_df.apply(lambda x: x + '\\n' if isinstance(x, str) else x)\n",
    "# \n",
    "# # Convert and save the training and testing data without quotes\n",
    "# train_df.to_csv('Restaurants.atepc.train.dat', index=False, header=False, sep='\\t', quoting=csv.QUOTE_NONE,escapechar=\"\\t\")\n",
    "# test_df.to_csv('Restaurants.atepc.test.dat', index=False, header=False, sep='\\t', quoting=csv.QUOTE_NONE,escapechar=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9dc7c12b97bed18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T12:26:21.693404Z",
     "start_time": "2024-07-14T12:26:21.623623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write the Series to a .dat file manually\n",
    "with open('Restaurants.atepc.train.dat', 'w') as f:\n",
    "    for item in train_df:\n",
    "        f.write(\"%s\\n\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Write the Series to a .dat file manually\n",
    "with open('Restaurants.atepc.test.dat', 'w') as f:\n",
    "    for item in test_df:\n",
    "        f.write(\"%s\\n\\n\" % item)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T12:26:24.236584Z",
     "start_time": "2024-07-14T12:26:24.202380Z"
    }
   },
   "id": "29ff3acd63cf383c",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd1b1ceae1b69716",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:08:51.011682Z",
     "start_time": "2024-06-23T17:08:47.343417Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Negative'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 65\u001B[0m\n\u001B[0;32m     62\u001B[0m all_aspects \u001B[38;5;241m=\u001B[39m get_all_aspects(data)\n\u001B[0;32m     64\u001B[0m \u001B[38;5;66;03m# Apply NLTK formatting to the DataFrame and display the results\u001B[39;00m\n\u001B[1;32m---> 65\u001B[0m example_data_nltk_formatted \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnltk_format_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_aspects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mall_aspects\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28mprint\u001B[39m(example_data_nltk_formatted)  \u001B[38;5;66;03m# Displaying formatted data for the first entry\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;66;03m# print(example_data_nltk_formatted.values[1])  # Displaying formatted data for \u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\HealthRiskPredictor\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:10361\u001B[0m, in \u001B[0;36mDataFrame.apply\u001B[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m  10347\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapply\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[0;32m  10349\u001B[0m op \u001B[38;5;241m=\u001B[39m frame_apply(\n\u001B[0;32m  10350\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m  10351\u001B[0m     func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m  10359\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[0;32m  10360\u001B[0m )\n\u001B[1;32m> 10361\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapply\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\HealthRiskPredictor\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001B[0m, in \u001B[0;36mFrameApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    913\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw:\n\u001B[0;32m    914\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_raw(engine\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine, engine_kwargs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine_kwargs)\n\u001B[1;32m--> 916\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\HealthRiskPredictor\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001B[0m, in \u001B[0;36mFrameApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1061\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1062\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpython\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m-> 1063\u001B[0m         results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1064\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1065\u001B[0m         results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_series_numba()\n",
      "File \u001B[1;32m~\\PycharmProjects\\HealthRiskPredictor\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001B[0m, in \u001B[0;36mFrameApply.apply_series_generator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1078\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.chained_assignment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   1079\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[0;32m   1080\u001B[0m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[1;32m-> 1081\u001B[0m         results[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1082\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[0;32m   1083\u001B[0m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[0;32m   1084\u001B[0m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[0;32m   1085\u001B[0m             results[i] \u001B[38;5;241m=\u001B[39m results[i]\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[22], line 48\u001B[0m, in \u001B[0;36mnltk_format_data\u001B[1;34m(row, all_aspects)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, aspect_token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(aspect_tokens):\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m j \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m---> 48\u001B[0m         formatted_sentence\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maspect_token\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m B-ASP \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpolarity_code\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mif\u001B[39;00m\u001B[38;5;250m \u001B[39mmatched_aspect\u001B[38;5;250m \u001B[39m\u001B[38;5;241m==\u001B[39m\u001B[38;5;250m \u001B[39mcurrent_aspect\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01melse\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-1\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43memotion_code\u001B[49m\u001B[43m[\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mEmotion Class\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mif\u001B[39;00m\u001B[38;5;250m \u001B[39mmatched_aspect\u001B[38;5;250m \u001B[39m\u001B[38;5;241m==\u001B[39m\u001B[38;5;250m \u001B[39mcurrent_aspect\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01melse\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-1\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     50\u001B[0m         formatted_sentence\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maspect_token\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m I-ASP \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpolarity_code\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mif\u001B[39;00m\u001B[38;5;250m \u001B[39mmatched_aspect\u001B[38;5;250m \u001B[39m\u001B[38;5;241m==\u001B[39m\u001B[38;5;250m \u001B[39mcurrent_aspect\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01melse\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-1\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00memotion_code[row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEmotion Class\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mif\u001B[39;00m\u001B[38;5;250m \u001B[39mmatched_aspect\u001B[38;5;250m \u001B[39m\u001B[38;5;241m==\u001B[39m\u001B[38;5;250m \u001B[39mcurrent_aspect\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01melse\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-1\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'Negative'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure nltk resources are available\n",
    "#nltk.download('punkt')\n",
    "\n",
    "def get_all_aspects(data):\n",
    "    aspect_dict = {}\n",
    "    for _, row in data.iterrows():\n",
    "        sentence = row['Review Sentence']\n",
    "        aspect = row['Aspect term']\n",
    "        if sentence not in aspect_dict:\n",
    "            aspect_dict[sentence] = []\n",
    "        aspect_dict[sentence].append(aspect)\n",
    "    return aspect_dict\n",
    "def nltk_format_data(row, all_aspects):\n",
    "    # Tokenize the sentence while correctly handling punctuation\n",
    "    tokens = word_tokenize(row['Review Sentence'])\n",
    "    current_aspect = row['Aspect term']\n",
    "    polarity_code = {'positive': 2, 'neutral': 0, 'negative': 1,'conflict':3}[row['polarity']]\n",
    "    \n",
    "    # Map emotion classes to numerical codes\n",
    "    emotion_code = {'Anger': 0, 'Disgust': 1, 'Fear': 2, 'Joy': 3, 'Sadness': 4, 'Surprise': 5} \n",
    "    \n",
    "    # Get all aspects for the current sentence\n",
    "    all_aspects_in_sentence = all_aspects[row['Review Sentence']]\n",
    "    \n",
    "    # Normalize the tokens to ensure consistent matching with aspect terms\n",
    "    normalized_tokens = [token.rstrip('.,?!:;') for token in tokens]\n",
    "    \n",
    "    formatted_sentence = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        normalized_token = normalized_tokens[i]\n",
    "        matched_aspect = None\n",
    "        \n",
    "        # Check if the token is part of any aspect term\n",
    "        for aspect in all_aspects_in_sentence:\n",
    "            aspect_tokens = aspect.split()\n",
    "            aspect_length = len(aspect_tokens)\n",
    "            if normalized_tokens[i:i+aspect_length] == aspect_tokens:\n",
    "                matched_aspect = aspect\n",
    "                break\n",
    "        \n",
    "        if matched_aspect:\n",
    "            aspect_tokens = matched_aspect.split()\n",
    "            for j, aspect_token in enumerate(aspect_tokens):\n",
    "                if j == 0:\n",
    "                    formatted_sentence.append(f\"{aspect_token} B-ASP {polarity_code if matched_aspect == current_aspect else '-1'} {emotion_code[row['Emotion Class']] if matched_aspect == current_aspect else '-1'}\")\n",
    "                else:\n",
    "                    formatted_sentence.append(f\"{aspect_token} I-ASP {polarity_code if matched_aspect == current_aspect else '-1'} {emotion_code[row['Emotion Class']] if matched_aspect == current_aspect else '-1'}\")\n",
    "            i += len(aspect_tokens)  # Skip the aspect tokens\n",
    "        else:\n",
    "            formatted_sentence.append(f\"{tokens[i]} O -1 -1\")\n",
    "            i += 1\n",
    "\n",
    "    return '\\n'.join(formatted_sentence)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get all aspects for each sentence\n",
    "all_aspects = get_all_aspects(data)\n",
    "\n",
    "# Apply NLTK formatting to the DataFrame and display the results\n",
    "example_data_nltk_formatted = data.apply(nltk_format_data, axis=1, all_aspects=all_aspects)\n",
    "print(example_data_nltk_formatted)  # Displaying formatted data for the first entry\n",
    "# print(example_data_nltk_formatted.values[1])  # Displaying formatted data for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87fc54c1fbeb6970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:36:01.760594Z",
     "start_time": "2024-06-14T14:35:51.598489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] although they do the typical what kind of water would you like questions the service was good and overall very relaxing to place [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "token_ids = [101, 2348, 2027, 2079, 1996, 5171, 2054, 2785, 1997, 2300, 2052, 2017, 2066, 3980, 1996, 2326, 2001, 2204, 1998, 3452, 2200, 19613, 2000, 2173, 102]\n",
    "\n",
    "# Convert token IDs to tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "# Join tokens to form the original text\n",
    "original_text = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8bc06961d7f778c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:39:16.465076Z",
     "start_time": "2024-06-14T14:39:16.449844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2348, 2027, 2079, 1996, 5171, 2054, 2785, 1997, 2300, 2052, 2017, 2066, 3980, 1996, 2326, 2001, 2204, 1998, 3452, 2200, 19613, 2000, 2173, 102]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"although they do the typical what kind of water would you like questions the service was good and overall very relaxing to place\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717ac3fe5c94d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
